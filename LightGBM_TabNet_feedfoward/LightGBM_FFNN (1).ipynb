{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp3cR-uQynHx"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import numpy.matlib\n",
        "\n",
        "\n",
        "path_submissions = '/'\n",
        "\n",
        "target_name = 'target'\n",
        "scores_folds = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F66O75ZWz777",
        "outputId": "ed8dae70-f1ce-479e-ae09-2c35469f7a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "!unzip gdrive/My\\ Drive/optiver-realized-volatility-prediction.zip > /dev/null\n",
        "\n",
        "!unzip gdrive/My\\ Drive/preprocessed.zip > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz9BDHHKLP--",
        "outputId": "81edaf8c-79c4-4b32-949b-f05d947e3009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/content/preprocessed/train_final.csv')\n",
        "test=pd.read_csv('/content/preprocessed/test_final.csv')\n"
      ],
      "metadata": {
        "id": "Zfeyc8sLz07G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data directory\n",
        "data_dir = '../content/optiver-realized-volatility-prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap4(df):\n",
        "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../content/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('../content/optiver-realized-volatility-prediction/test.csv')\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n",
        "\n",
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    df['wap3'] = calc_wap3(df)\n",
        "    df['wap4'] = calc_wap4(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
        "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.std],\n",
        "        'wap2': [np.sum, np.std],\n",
        "        'wap3': [np.sum, np.std],\n",
        "        'wap4': [np.sum, np.std],\n",
        "        'log_return1': [realized_volatility],\n",
        "        'log_return2': [realized_volatility],\n",
        "        'log_return3': [realized_volatility],\n",
        "        'log_return4': [realized_volatility],\n",
        "        'wap_balance': [np.sum, np.max],\n",
        "        'price_spread':[np.sum, np.max],\n",
        "        'price_spread2':[np.sum, np.max],\n",
        "        'bid_spread':[np.sum, np.max],\n",
        "        'ask_spread':[np.sum, np.max],\n",
        "        'total_volume':[np.sum, np.max],\n",
        "        'volume_imbalance':[np.sum, np.max],\n",
        "        \"bid_ask_spread\":[np.sum,  np.max],\n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return1': [realized_volatility],\n",
        "        'log_return2': [realized_volatility],\n",
        "        'log_return3': [realized_volatility],\n",
        "        'log_return4': [realized_volatility],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
        "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
        "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
        "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
        "    \n",
        "    \n",
        "    # Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    df['amount']=df['price']*df['size']\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum, np.max, np.min],\n",
        "        'order_count':[np.sum,np.max],\n",
        "        'amount':[np.sum,np.max,np.min],\n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.sum],\n",
        "    }\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "\n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
        "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
        "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
        "    \n",
        "    def tendency(price, vol):    \n",
        "        df_diff = np.diff(price)\n",
        "        val = (df_diff/price[1:])*100\n",
        "        power = np.sum(val*vol[1:])\n",
        "        return(power)\n",
        "    \n",
        "    lis = []\n",
        "    for n_time_id in df['time_id'].unique():\n",
        "        df_id = df[df['time_id'] == n_time_id]        \n",
        "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
        "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
        "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
        "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
        "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
        "        # new\n",
        "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
        "        energy = np.mean(df_id['price'].values**2)\n",
        "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
        "        \n",
        "        # vol vars\n",
        "        \n",
        "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
        "        energy_v = np.sum(df_id['size'].values**2)\n",
        "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
        "        \n",
        "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
        "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
        "    \n",
        "    df_lr = pd.DataFrame(lis)\n",
        "        \n",
        "   \n",
        "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
        "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
        "    \n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature\n",
        "\n",
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
        "\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    return df\n",
        "    \n",
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "metadata": {
        "id": "GjyGbKYH7fWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read train and test\n",
        "train, test = read_train_test()\n",
        "\n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get group stats of time_id and stock_id\n",
        "train = get_time_stock(train)\n",
        "test = get_time_stock(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAskFJdu8UPN",
        "outputId": "e2ed97d8-7d13-4771-9812-1af0af9b73bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our training set has 428932 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 25.3min\n",
            "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 61.7min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace by order sum (tau)\n",
        "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
        "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
        "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
        "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
        "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
        "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
        "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
        "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
        "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
        "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
        "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
        "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
      ],
      "metadata": {
        "id": "r8dIEWCo8X91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
        "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
        "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
        "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
        "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
        "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
        "\n",
        "# delta tau\n",
        "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
        "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
      ],
      "metadata": {
        "id": "K0U_79pxRT59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.dtypes"
      ],
      "metadata": {
        "id": "8XSI8UqX1SFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9f3968-2fb1-4e75-85f6-b93e09d7546f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "stock_id           int64\n",
              "time_id            int64\n",
              "target           float64\n",
              "row_id            object\n",
              "wap1_sum         float64\n",
              "                  ...   \n",
              "size_tau2        float64\n",
              "size_tau2_400    float64\n",
              "size_tau2_300    float64\n",
              "size_tau2_200    float64\n",
              "size_tau2_d      float64\n",
              "Length: 198, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colNames = [col for col in list(train.columns)\n",
        "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
        "len(colNames)"
      ],
      "metadata": {
        "id": "-BO1uUzlI4ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd05f51a-e53b-46d3-85d4-5faa917bcfc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "194"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# making agg features\n",
        "\n",
        "train_p = pd.read_csv('/content/optiver-realized-volatility-prediction/train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(7):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "\n",
        "mat2 = pd.concat(matTest).reset_index()"
      ],
      "metadata": {
        "id": "vDhmfUJHJA3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43eb47e4-428b-46c4-dc48-a26a7f68bdcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
            " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
            " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
            " 4]\n",
            "[5, 10, 22, 23, 29, 36, 44, 48, 56, 66, 69, 72, 73, 76, 87, 94, 95, 102, 109, 112, 113, 115, 116, 120, 122]\n",
            "[3, 6, 9, 18, 61, 63]\n",
            "[81]\n",
            "[0, 2, 4, 7, 13, 14, 15, 16, 17, 19, 20, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 85, 93, 100, 103, 104, 105, 107, 114, 118, 119, 123, 125]\n",
            "[1, 11, 37, 50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124, 126]\n",
            "[8, 80]\n",
            "[21, 27, 31, 33, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 108, 110, 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
        "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "mat1.reset_index(inplace=True)\n",
        "\n",
        "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "mat2.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "qcTiFMwpJBpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8e589d-d8bd-4f2f-9682-94bef6157346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-59019010fc55>:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
            "<ipython-input-12-59019010fc55>:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnn = ['time_id',\n",
        "     'log_return1_realized_volatility_0c1',\n",
        "     'log_return1_realized_volatility_1c1',     \n",
        "     'log_return1_realized_volatility_3c1',\n",
        "     'log_return1_realized_volatility_4c1',     \n",
        "     'log_return1_realized_volatility_6c1',\n",
        "     'total_volume_sum_0c1',\n",
        "     'total_volume_sum_1c1', \n",
        "     'total_volume_sum_3c1',\n",
        "     'total_volume_sum_4c1', \n",
        "     'total_volume_sum_6c1',\n",
        "     'trade_size_sum_0c1',\n",
        "     'trade_size_sum_1c1', \n",
        "     'trade_size_sum_3c1',\n",
        "     'trade_size_sum_4c1', \n",
        "     'trade_size_sum_6c1',\n",
        "     'trade_order_count_sum_0c1',\n",
        "     'trade_order_count_sum_1c1',\n",
        "     'trade_order_count_sum_3c1',\n",
        "     'trade_order_count_sum_4c1',\n",
        "     'trade_order_count_sum_6c1',      \n",
        "     'price_spread_sum_0c1',\n",
        "     'price_spread_sum_1c1',\n",
        "     'price_spread_sum_3c1',\n",
        "     'price_spread_sum_4c1',\n",
        "     'price_spread_sum_6c1',   \n",
        "     'bid_spread_sum_0c1',\n",
        "     'bid_spread_sum_1c1',\n",
        "     'bid_spread_sum_3c1',\n",
        "     'bid_spread_sum_4c1',\n",
        "     'bid_spread_sum_6c1',       \n",
        "     'ask_spread_sum_0c1',\n",
        "     'ask_spread_sum_1c1',\n",
        "     'ask_spread_sum_3c1',\n",
        "     'ask_spread_sum_4c1',\n",
        "     'ask_spread_sum_6c1',   \n",
        "     'volume_imbalance_sum_0c1',\n",
        "     'volume_imbalance_sum_1c1',\n",
        "     'volume_imbalance_sum_3c1',\n",
        "     'volume_imbalance_sum_4c1',\n",
        "     'volume_imbalance_sum_6c1',       \n",
        "     'bid_ask_spread_sum_0c1',\n",
        "     'bid_ask_spread_sum_1c1',\n",
        "     'bid_ask_spread_sum_3c1',\n",
        "     'bid_ask_spread_sum_4c1',\n",
        "     'bid_ask_spread_sum_6c1',\n",
        "     'size_tau2_0c1',\n",
        "     'size_tau2_1c1',\n",
        "     'size_tau2_3c1',\n",
        "     'size_tau2_4c1',\n",
        "     'size_tau2_6c1'] "
      ],
      "metadata": {
        "id": "qwPfW-ycJWU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
        "test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n",
        "import gc\n",
        "del mat1,mat2\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vEKq-6jJYV1",
        "outputId": "1df98de2-8fff-49db-97e5-b6d19bb00914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1434"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "seed0=2021\n",
        "params0 = {\n",
        "    'objective': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_depth': -1,\n",
        "    'max_bin':100,\n",
        "    'min_data_in_leaf':500,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.72,\n",
        "    'subsample_freq': 4,\n",
        "    'feature_fraction': 0.5,\n",
        "    'lambda_l1': 0.5,\n",
        "    'lambda_l2': 1.0,\n",
        "    'categorical_column':[0],\n",
        "    'seed':seed0,\n",
        "    'feature_fraction_seed': seed0,\n",
        "    'bagging_seed': seed0,\n",
        "    'drop_seed': seed0,\n",
        "    'data_random_seed': seed0,\n",
        "    'n_jobs':-1,\n",
        "    'verbose': -1}\n",
        "seed1=42\n",
        "params1 = {\n",
        "        'learning_rate': 0.1,        \n",
        "        'lambda_l1': 2,\n",
        "        'lambda_l2': 7,\n",
        "        'num_leaves': 800,\n",
        "        'min_sum_hessian_in_leaf': 20,\n",
        "        'feature_fraction': 0.8,\n",
        "        'feature_fraction_bynode': 0.8,\n",
        "        'bagging_fraction': 0.9,\n",
        "        'bagging_freq': 42,\n",
        "        'min_data_in_leaf': 700,\n",
        "        'max_depth': 4,\n",
        "        'categorical_column':[0],\n",
        "        'seed': seed1,\n",
        "        'feature_fraction_seed': seed1,\n",
        "        'bagging_seed': seed1,\n",
        "        'drop_seed': seed1,\n",
        "        'data_random_seed': seed1,\n",
        "        'objective': 'rmse',\n",
        "        'boosting': 'gbdt',\n",
        "        'verbosity': -1,\n",
        "        'n_jobs':-1,\n",
        "    }\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
        "\n",
        "def train_and_evaluate_lgb(train, test, params):\n",
        "    # Hyperparammeters (just basic)\n",
        "    \n",
        "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "    y = train['target']\n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(train.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
        "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
        "        model = lgb.train(params = params,\n",
        "                          num_boost_round=1000,\n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          verbose_eval = 250,\n",
        "                          early_stopping_rounds=50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(test[features]) / 5\n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "    lgb.plot_importance(model,max_num_features=20)\n",
        "    # Return test predictions\n",
        "    return test_predictions,oof_predictions\n"
      ],
      "metadata": {
        "id": "Tq3KvuBJJbME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JOR4PzqJe5V",
        "outputId": "7680ea17-45fa-415d-cb58-45fd62aee8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "248"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traing and evaluate\n",
        "predictions_lgb,oof_predictions= train_and_evaluate_lgb(train, test,params0)\n",
        "test['target'] = predictions_lgb\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-ilPvhrx_r3v",
        "outputId": "6b0a78fa-d226-482f-8425-8995a2afa290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[250]\ttraining's rmse: 0.00042901\ttraining's RMSPE: 0.198408\tvalid_1's rmse: 0.000440822\tvalid_1's RMSPE: 0.204605\n",
            "[500]\ttraining's rmse: 0.000406828\ttraining's RMSPE: 0.18815\tvalid_1's rmse: 0.000426529\tvalid_1's RMSPE: 0.197971\n",
            "[750]\ttraining's rmse: 0.000393369\ttraining's RMSPE: 0.181925\tvalid_1's rmse: 0.000419133\tvalid_1's RMSPE: 0.194538\n",
            "[1000]\ttraining's rmse: 0.000383721\ttraining's RMSPE: 0.177463\tvalid_1's rmse: 0.000416015\tvalid_1's RMSPE: 0.19309\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.000383721\ttraining's RMSPE: 0.177463\tvalid_1's rmse: 0.000416015\tvalid_1's RMSPE: 0.19309\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[250]\ttraining's rmse: 0.000428793\ttraining's RMSPE: 0.198655\tvalid_1's rmse: 0.000440415\tvalid_1's RMSPE: 0.20299\n",
            "[500]\ttraining's rmse: 0.000406565\ttraining's RMSPE: 0.188357\tvalid_1's rmse: 0.000425261\tvalid_1's RMSPE: 0.196006\n",
            "[750]\ttraining's rmse: 0.000393057\ttraining's RMSPE: 0.182098\tvalid_1's rmse: 0.000417537\tvalid_1's RMSPE: 0.192446\n",
            "[1000]\ttraining's rmse: 0.000382931\ttraining's RMSPE: 0.177407\tvalid_1's rmse: 0.000412862\tvalid_1's RMSPE: 0.190291\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.000382931\ttraining's RMSPE: 0.177407\tvalid_1's rmse: 0.000412862\tvalid_1's RMSPE: 0.190291\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[250]\ttraining's rmse: 0.000429235\ttraining's RMSPE: 0.198505\tvalid_1's rmse: 0.000464597\tvalid_1's RMSPE: 0.21567\n",
            "[500]\ttraining's rmse: 0.000406613\ttraining's RMSPE: 0.188044\tvalid_1's rmse: 0.000449465\tvalid_1's RMSPE: 0.208645\n",
            "[750]\ttraining's rmse: 0.000392538\ttraining's RMSPE: 0.181534\tvalid_1's rmse: 0.000441265\tvalid_1's RMSPE: 0.204839\n",
            "[1000]\ttraining's rmse: 0.000382773\ttraining's RMSPE: 0.177019\tvalid_1's rmse: 0.000437066\tvalid_1's RMSPE: 0.202889\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.000382773\ttraining's RMSPE: 0.177019\tvalid_1's rmse: 0.000437066\tvalid_1's RMSPE: 0.202889\n",
            "Training fold 4\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[250]\ttraining's rmse: 0.000428307\ttraining's RMSPE: 0.19843\tvalid_1's rmse: 0.000445917\tvalid_1's RMSPE: 0.205525\n",
            "[500]\ttraining's rmse: 0.000406918\ttraining's RMSPE: 0.18852\tvalid_1's rmse: 0.000431223\tvalid_1's RMSPE: 0.198752\n",
            "[750]\ttraining's rmse: 0.000393107\ttraining's RMSPE: 0.182122\tvalid_1's rmse: 0.00042263\tvalid_1's RMSPE: 0.194792\n",
            "[1000]\ttraining's rmse: 0.000383178\ttraining's RMSPE: 0.177522\tvalid_1's rmse: 0.000417683\tvalid_1's RMSPE: 0.192511\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.000383178\ttraining's RMSPE: 0.177522\tvalid_1's rmse: 0.000417683\tvalid_1's RMSPE: 0.192511\n",
            "Training fold 5\n",
            "Training until validation scores don't improve for 50 rounds.\n",
            "[250]\ttraining's rmse: 0.000429791\ttraining's RMSPE: 0.198797\tvalid_1's rmse: 0.000441959\tvalid_1's RMSPE: 0.20502\n",
            "[500]\ttraining's rmse: 0.000408015\ttraining's RMSPE: 0.188724\tvalid_1's rmse: 0.000427561\tvalid_1's RMSPE: 0.198341\n",
            "[750]\ttraining's rmse: 0.000393929\ttraining's RMSPE: 0.182209\tvalid_1's rmse: 0.000419326\tvalid_1's RMSPE: 0.194521\n",
            "[1000]\ttraining's rmse: 0.000384071\ttraining's RMSPE: 0.17765\tvalid_1's rmse: 0.000415068\tvalid_1's RMSPE: 0.192545\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\ttraining's rmse: 0.000384071\ttraining's RMSPE: 0.17765\tvalid_1's rmse: 0.000415068\tvalid_1's RMSPE: 0.192545\n",
            "Our out of folds RMSPE is 0.19431574325011014\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "def root_mean_squared_per_error(y_true, y_pred):\n",
        "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
        "    \n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=20, verbose=0,\n",
        "    mode='min',restore_best_weights=True)\n",
        "\n",
        "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
        "    mode='min')"
      ],
      "metadata": {
        "id": "AC-7_T4VJigG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kfold based on the knn++ algorithm\n",
        "\n",
        "out_train = pd.read_csv('../content/optiver-realized-volatility-prediction/train.csv')\n",
        "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "#out_train[out_train.isna().any(axis=1)]\n",
        "out_train = out_train.fillna(out_train.mean())\n",
        "out_train.head()\n",
        "\n",
        "# code to add the just the read data after first execution\n",
        "\n",
        "# data separation based on knn ++\n",
        "nfolds = 5 # number of folds\n",
        "index = []\n",
        "totDist = []\n",
        "values = []\n",
        "# generates a matriz with the values of \n",
        "mat = out_train.values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "mat = scaler.fit_transform(mat)\n",
        "\n",
        "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
        "\n",
        "# adds index in the last column\n",
        "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
        "\n",
        "\n",
        "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
        "\n",
        "lineNumber = np.sort(lineNumber)[::-1]\n",
        "\n",
        "for n in range(nfolds):\n",
        "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
        "\n",
        "# saves index\n",
        "for n in range(nfolds):\n",
        "    \n",
        "    values.append([lineNumber[n]])    \n",
        "\n",
        "\n",
        "s=[]\n",
        "for n in range(nfolds):\n",
        "    s.append(mat[lineNumber[n],:])\n",
        "    \n",
        "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
        "\n",
        "for n in range(nind-1):    \n",
        "\n",
        "    luck = np.random.uniform(0,1,nfolds)\n",
        "    \n",
        "    for cycle in range(nfolds):\n",
        "         # saves the values of index           \n",
        "\n",
        "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
        "\n",
        "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
        "        totDist[cycle] += sumDist        \n",
        "                \n",
        "        # probabilities\n",
        "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
        "        j = 0\n",
        "        kn = 0\n",
        "        for val in f:\n",
        "            j += val        \n",
        "            if (j > luck[cycle]): # the column was selected\n",
        "                break\n",
        "            kn +=1\n",
        "        lineNumber[cycle] = kn\n",
        "        \n",
        "        # delete line of the value added    \n",
        "        for n_iter in range(nfolds):\n",
        "            \n",
        "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
        "            j= 0\n",
        "        \n",
        "        s[cycle] = mat[lineNumber[cycle],:]\n",
        "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
        "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
        "\n",
        "\n",
        "for n_mod in range(nfolds):\n",
        "    values[n_mod] = out_train.index[values[n_mod]]"
      ],
      "metadata": {
        "id": "pLMHmzZVNJD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#colNames.remove('row_id')\n",
        "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "qt_train = []\n",
        "train_nn=train[colNames].copy()\n",
        "test_nn=test[colNames].copy()\n",
        "for col in colNames:\n",
        "    #print(col)\n",
        "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
        "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
        "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
        "    qt_train.append(qt)"
      ],
      "metadata": {
        "id": "Hcrpl04YNTg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
        "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
      ],
      "metadata": {
        "id": "Xm0IxoC8Ogoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making agg features\n",
        "from sklearn.cluster import KMeans\n",
        "train_p = pd.read_csv('/content/optiver-realized-volatility-prediction/train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(7):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "\n",
        "mat2 = pd.concat(matTest).reset_index()\n",
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZWamX99NWLa",
        "outputId": "c03f6886-182f-45cc-af4a-b8ebc729f73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
            " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
            " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
            " 4]\n",
            "[5, 10, 22, 23, 29, 36, 44, 48, 56, 66, 69, 72, 73, 76, 87, 94, 95, 102, 109, 112, 113, 115, 116, 120, 122]\n",
            "[3, 6, 9, 18, 61, 63]\n",
            "[81]\n",
            "[0, 2, 4, 7, 13, 14, 15, 16, 17, 19, 20, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 85, 93, 100, 103, 104, 105, 107, 114, 118, 119, 123, 125]\n",
            "[1, 11, 37, 50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124, 126]\n",
            "[8, 80]\n",
            "[21, 27, 31, 33, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 108, 110, 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#colNames.remove('row_id')\n",
        "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "qt_train = []\n",
        "train_nn=train[colNames].copy()\n",
        "test_nn=test[colNames].copy()\n",
        "for col in colNames:\n",
        "    #print(col)\n",
        "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
        "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
        "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
        "    qt_train.append(qt)"
      ],
      "metadata": {
        "id": "GmWX9ic5NY1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
        "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
      ],
      "metadata": {
        "id": "UDm8mzadOlbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making agg features\n",
        "from sklearn.cluster import KMeans\n",
        "train_p = pd.read_csv('/content/optiver-realized-volatility-prediction/train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(7):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "\n",
        "mat2 = pd.concat(matTest).reset_index()\n",
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZbcFlPkNbqq",
        "outputId": "171c6089-b2cf-4da3-bf7c-f19f3f695315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3\n",
            " 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4\n",
            " 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3\n",
            " 4]\n",
            "[5, 10, 22, 23, 29, 36, 44, 48, 56, 66, 69, 72, 73, 76, 87, 94, 95, 102, 109, 112, 113, 115, 116, 120, 122]\n",
            "[3, 6, 9, 18, 61, 63]\n",
            "[81]\n",
            "[0, 2, 4, 7, 13, 14, 15, 16, 17, 19, 20, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 85, 93, 100, 103, 104, 105, 107, 114, 118, 119, 123, 125]\n",
            "[1, 11, 37, 50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124, 126]\n",
            "[8, 80]\n",
            "[21, 27, 31, 33, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 108, 110, 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnn = ['time_id',\n",
        "     'log_return1_realized_volatility_0c1',\n",
        "     'log_return1_realized_volatility_1c1',     \n",
        "     'log_return1_realized_volatility_3c1',\n",
        "     'log_return1_realized_volatility_4c1',     \n",
        "     'log_return1_realized_volatility_6c1',\n",
        "     'total_volume_sum_0c1',\n",
        "     'total_volume_sum_1c1', \n",
        "     'total_volume_sum_3c1',\n",
        "     'total_volume_sum_4c1', \n",
        "     'total_volume_sum_6c1',\n",
        "     'trade_size_sum_0c1',\n",
        "     'trade_size_sum_1c1', \n",
        "     'trade_size_sum_3c1',\n",
        "     'trade_size_sum_4c1', \n",
        "     'trade_size_sum_6c1',\n",
        "     'trade_order_count_sum_0c1',\n",
        "     'trade_order_count_sum_1c1',\n",
        "     'trade_order_count_sum_3c1',\n",
        "     'trade_order_count_sum_4c1',\n",
        "     'trade_order_count_sum_6c1',      \n",
        "     'price_spread_sum_0c1',\n",
        "     'price_spread_sum_1c1',\n",
        "     'price_spread_sum_3c1',\n",
        "     'price_spread_sum_4c1',\n",
        "     'price_spread_sum_6c1',   \n",
        "     'bid_spread_sum_0c1',\n",
        "     'bid_spread_sum_1c1',\n",
        "     'bid_spread_sum_3c1',\n",
        "     'bid_spread_sum_4c1',\n",
        "     'bid_spread_sum_6c1',       \n",
        "     'ask_spread_sum_0c1',\n",
        "     'ask_spread_sum_1c1',\n",
        "     'ask_spread_sum_3c1',\n",
        "     'ask_spread_sum_4c1',\n",
        "     'ask_spread_sum_6c1',   \n",
        "     'volume_imbalance_sum_0c1',\n",
        "     'volume_imbalance_sum_1c1',\n",
        "     'volume_imbalance_sum_3c1',\n",
        "     'volume_imbalance_sum_4c1',\n",
        "     'volume_imbalance_sum_6c1',       \n",
        "     'bid_ask_spread_sum_0c1',\n",
        "     'bid_ask_spread_sum_1c1',\n",
        "     'bid_ask_spread_sum_3c1',\n",
        "     'bid_ask_spread_sum_4c1',\n",
        "     'bid_ask_spread_sum_6c1',\n",
        "     'size_tau2_0c1',\n",
        "     'size_tau2_1c1',\n",
        "     'size_tau2_3c1',\n",
        "     'size_tau2_4c1',\n",
        "     'size_tau2_6c1'] "
      ],
      "metadata": {
        "id": "oMrp0V77NeNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "mat1.reset_index(inplace=True)\n",
        "\n",
        "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "mat2.reset_index(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUm-ZVCRO0UF",
        "outputId": "89328719-4e63-476f-b1ba-51ccfe7ac801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-79e19df2b5ab>:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
            "<ipython-input-36-79e19df2b5ab>:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
        "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
        "del mat1,mat2\n",
        "del train,test\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9dtiAzXO3RI",
        "outputId": "9b6a054e-b007-4d3f-d21d-7a90c30f6a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
        "from keras.backend import sigmoid\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "get_custom_objects().update({'swish': Activation(swish)})\n",
        "\n",
        "hidden_units = (128,64,32)\n",
        "stock_embedding_size = 24\n",
        "\n",
        "cat_data = train_nn['stock_id']\n",
        "\n",
        "def base_model():\n",
        "    \n",
        "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
        "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
        "    num_input = keras.Input(shape=(244,), name='num_data')\n",
        "\n",
        "\n",
        "    #embedding, flatenning and concatenating\n",
        "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
        "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
        "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
        "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
        "    \n",
        "    # Add one or more hidden layers\n",
        "    for n_hidden in hidden_units:\n",
        "\n",
        "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
        "        \n",
        "\n",
        "    #out = keras.layers.Concatenate()([out, num_input])\n",
        "\n",
        "    # A single output: our predicted rating\n",
        "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
        "    \n",
        "    model = keras.Model(\n",
        "    inputs = [stock_id_input, num_input],\n",
        "    outputs = out,\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "z0obJACMO6Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "metadata": {
        "id": "jqb0sqFBO-Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_name='target'\n",
        "scores_folds = {}\n",
        "model_name = 'NN'\n",
        "pred_name = 'pred_{}'.format(model_name)\n",
        "\n",
        "n_folds = 5\n",
        "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
        "scores_folds[model_name] = []\n",
        "counter = 1\n",
        "\n",
        "features_to_consider = list(train_nn)\n",
        "\n",
        "features_to_consider.remove('time_id')\n",
        "features_to_consider.remove('target')\n",
        "try:\n",
        "    features_to_consider.remove('pred_NN')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "\n",
        "train_nn[pred_name] = 0\n",
        "test_nn[target_name] = 0\n",
        "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
        "\n",
        "for n_count in range(n_folds):\n",
        "    print('CV {}/{}'.format(counter, n_folds))\n",
        "    \n",
        "    indexes = np.arange(nfolds).astype(int)    \n",
        "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
        "    \n",
        "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
        "    \n",
        "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
        "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
        "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
        "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
        "    \n",
        "    #############################################################################################\n",
        "    # NN\n",
        "    #############################################################################################\n",
        "    \n",
        "    model = base_model()\n",
        "    \n",
        "    model.compile(\n",
        "        keras.optimizers.Adam(learning_rate=0.006),\n",
        "        loss=root_mean_squared_per_error\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        features_to_consider.remove('stock_id')\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    num_data = X_train[features_to_consider]\n",
        "    \n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
        "    num_data = scaler.fit_transform(num_data.values)    \n",
        "    \n",
        "    cat_data = X_train['stock_id']    \n",
        "    target =  y_train\n",
        "    \n",
        "    num_data_test = X_test[features_to_consider]\n",
        "    num_data_test = scaler.transform(num_data_test.values)\n",
        "    cat_data_test = X_test['stock_id']\n",
        "\n",
        "    model.fit([cat_data, num_data], \n",
        "              target,               \n",
        "              batch_size=2048,\n",
        "              epochs=1000,\n",
        "              validation_data=([cat_data_test, num_data_test], y_test),\n",
        "              callbacks=[es, plateau],\n",
        "              validation_batch_size=len(y_test),\n",
        "              shuffle=True,\n",
        "             verbose = 1)\n",
        "\n",
        "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
        "    \n",
        "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
        "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
        "    scores_folds[model_name].append(score)\n",
        "    \n",
        "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
        "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
        "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
        "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
        "       \n",
        "    counter += 1\n",
        "    features_to_consider.append('stock_id')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hbGc7D1PA4s",
        "outputId": "248c0f08-da8f-4b63-c24a-8519b7928792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV 1/5\n",
            "Epoch 1/1000\n",
            "51/51 [==============================] - 3s 30ms/step - loss: 19.5806 - val_loss: 3.9442 - lr: 0.0060\n",
            "Epoch 2/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 1.9204 - val_loss: 0.7993 - lr: 0.0060\n",
            "Epoch 3/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 1.0982 - val_loss: 0.7281 - lr: 0.0060\n",
            "Epoch 4/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 1.2965 - val_loss: 0.9283 - lr: 0.0060\n",
            "Epoch 5/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 1.1495 - val_loss: 0.4977 - lr: 0.0060\n",
            "Epoch 6/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.7528 - val_loss: 0.5963 - lr: 0.0060\n",
            "Epoch 7/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.6856 - val_loss: 0.8057 - lr: 0.0060\n",
            "Epoch 8/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.6027 - val_loss: 0.9415 - lr: 0.0060\n",
            "Epoch 9/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.4820 - val_loss: 0.1749 - lr: 0.0060\n",
            "Epoch 10/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.5977 - val_loss: 0.9516 - lr: 0.0060\n",
            "Epoch 11/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.6064 - val_loss: 0.5415 - lr: 0.0060\n",
            "Epoch 12/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.7757 - val_loss: 1.1684 - lr: 0.0060\n",
            "Epoch 13/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.8370 - val_loss: 0.6196 - lr: 0.0060\n",
            "Epoch 14/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.5796 - val_loss: 0.5192 - lr: 0.0060\n",
            "Epoch 15/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.5333 - val_loss: 0.3237 - lr: 0.0060\n",
            "Epoch 16/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.7574 - val_loss: 0.5606 - lr: 0.0060\n",
            "Epoch 17/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1866 - val_loss: 0.1171 - lr: 0.0012\n",
            "Epoch 18/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1114 - val_loss: 0.1126 - lr: 0.0012\n",
            "Epoch 19/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1087 - val_loss: 0.1139 - lr: 0.0012\n",
            "Epoch 20/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1125 - val_loss: 0.1227 - lr: 0.0012\n",
            "Epoch 21/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1040 - val_loss: 0.1233 - lr: 0.0012\n",
            "Epoch 22/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1039 - val_loss: 0.1105 - lr: 0.0012\n",
            "Epoch 23/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1006 - val_loss: 0.1204 - lr: 0.0012\n",
            "Epoch 24/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1008 - val_loss: 0.1436 - lr: 0.0012\n",
            "Epoch 25/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1071 - val_loss: 0.1536 - lr: 0.0012\n",
            "Epoch 26/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1078 - val_loss: 0.1086 - lr: 0.0012\n",
            "Epoch 27/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1027 - val_loss: 0.1354 - lr: 0.0012\n",
            "Epoch 28/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1013 - val_loss: 0.1390 - lr: 0.0012\n",
            "Epoch 29/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1016 - val_loss: 0.1125 - lr: 0.0012\n",
            "Epoch 30/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1000 - val_loss: 0.1256 - lr: 0.0012\n",
            "Epoch 31/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1581 - val_loss: 0.1983 - lr: 0.0012\n",
            "Epoch 32/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1522 - val_loss: 0.2217 - lr: 0.0012\n",
            "Epoch 33/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1143 - val_loss: 0.1010 - lr: 0.0012\n",
            "Epoch 34/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0952 - val_loss: 0.1475 - lr: 0.0012\n",
            "Epoch 35/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.1017 - val_loss: 0.1250 - lr: 0.0012\n",
            "Epoch 36/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0971 - val_loss: 0.1042 - lr: 0.0012\n",
            "Epoch 37/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1097 - val_loss: 0.1665 - lr: 0.0012\n",
            "Epoch 38/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1052 - val_loss: 0.1294 - lr: 0.0012\n",
            "Epoch 39/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1068 - val_loss: 0.1224 - lr: 0.0012\n",
            "Epoch 40/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1666 - val_loss: 0.1512 - lr: 0.0012\n",
            "Epoch 41/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0954 - val_loss: 0.0927 - lr: 2.4000e-04\n",
            "Epoch 42/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0878 - val_loss: 0.0966 - lr: 2.4000e-04\n",
            "Epoch 43/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0865 - val_loss: 0.1000 - lr: 2.4000e-04\n",
            "Epoch 44/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0893 - val_loss: 0.0920 - lr: 2.4000e-04\n",
            "Epoch 45/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0857 - val_loss: 0.0914 - lr: 2.4000e-04\n",
            "Epoch 46/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0856 - val_loss: 0.0955 - lr: 2.4000e-04\n",
            "Epoch 47/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0864 - val_loss: 0.0911 - lr: 2.4000e-04\n",
            "Epoch 48/1000\n",
            "51/51 [==============================] - 2s 40ms/step - loss: 0.0856 - val_loss: 0.0964 - lr: 2.4000e-04\n",
            "Epoch 49/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0860 - val_loss: 0.0911 - lr: 2.4000e-04\n",
            "Epoch 50/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0855 - val_loss: 0.0911 - lr: 2.4000e-04\n",
            "Epoch 51/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0846 - val_loss: 0.1141 - lr: 2.4000e-04\n",
            "Epoch 52/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0876 - val_loss: 0.1014 - lr: 2.4000e-04\n",
            "Epoch 53/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0862 - val_loss: 0.1016 - lr: 2.4000e-04\n",
            "Epoch 54/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0860 - val_loss: 0.0965 - lr: 2.4000e-04\n",
            "Epoch 55/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0843 - val_loss: 0.0895 - lr: 4.8000e-05\n",
            "Epoch 56/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0835 - val_loss: 0.0895 - lr: 4.8000e-05\n",
            "Epoch 57/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0835 - val_loss: 0.0898 - lr: 4.8000e-05\n",
            "Epoch 58/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0834 - val_loss: 0.0896 - lr: 4.8000e-05\n",
            "Epoch 59/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0834 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 60/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0834 - val_loss: 0.0903 - lr: 4.8000e-05\n",
            "Epoch 61/1000\n",
            "51/51 [==============================] - 2s 42ms/step - loss: 0.0840 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 62/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0834 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 63/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0833 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 64/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0832 - val_loss: 0.0896 - lr: 4.8000e-05\n",
            "Epoch 65/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0833 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 66/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0834 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 67/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0833 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 68/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0833 - val_loss: 0.0890 - lr: 4.8000e-05\n",
            "Epoch 69/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0832 - val_loss: 0.0891 - lr: 4.8000e-05\n",
            "Epoch 70/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0832 - val_loss: 0.0890 - lr: 4.8000e-05\n",
            "Epoch 71/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0830 - val_loss: 0.0891 - lr: 4.8000e-05\n",
            "Epoch 72/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0831 - val_loss: 0.0904 - lr: 4.8000e-05\n",
            "Epoch 73/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0831 - val_loss: 0.0888 - lr: 4.8000e-05\n",
            "Epoch 74/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0831 - val_loss: 0.0888 - lr: 4.8000e-05\n",
            "Epoch 75/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0830 - val_loss: 0.0888 - lr: 4.8000e-05\n",
            "Epoch 76/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0830 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 77/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0829 - val_loss: 0.0887 - lr: 4.8000e-05\n",
            "Epoch 78/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0831 - val_loss: 0.0893 - lr: 4.8000e-05\n",
            "Epoch 79/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0828 - val_loss: 0.0914 - lr: 4.8000e-05\n",
            "Epoch 80/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0832 - val_loss: 0.0900 - lr: 4.8000e-05\n",
            "Epoch 81/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0832 - val_loss: 0.0891 - lr: 4.8000e-05\n",
            "Epoch 82/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0828 - val_loss: 0.0897 - lr: 4.8000e-05\n",
            "Epoch 83/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0827 - val_loss: 0.0898 - lr: 4.8000e-05\n",
            "Epoch 84/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0826 - val_loss: 0.0886 - lr: 4.8000e-05\n",
            "Epoch 85/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0830 - val_loss: 0.0890 - lr: 4.8000e-05\n",
            "Epoch 86/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0830 - val_loss: 0.0904 - lr: 4.8000e-05\n",
            "Epoch 87/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0827 - val_loss: 0.0898 - lr: 4.8000e-05\n",
            "Epoch 88/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0828 - val_loss: 0.0897 - lr: 4.8000e-05\n",
            "Epoch 89/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0830 - val_loss: 0.0883 - lr: 4.8000e-05\n",
            "Epoch 90/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0823 - val_loss: 0.0882 - lr: 4.8000e-05\n",
            "Epoch 91/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0823 - val_loss: 0.0883 - lr: 4.8000e-05\n",
            "Epoch 92/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0824 - val_loss: 0.0886 - lr: 4.8000e-05\n",
            "Epoch 93/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0821 - val_loss: 0.0881 - lr: 4.8000e-05\n",
            "Epoch 94/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0820 - val_loss: 0.0887 - lr: 4.8000e-05\n",
            "Epoch 95/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0822 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 96/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0818 - val_loss: 0.0880 - lr: 4.8000e-05\n",
            "Epoch 97/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0820 - val_loss: 0.0889 - lr: 4.8000e-05\n",
            "Epoch 98/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0834 - val_loss: 0.0883 - lr: 4.8000e-05\n",
            "Epoch 99/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0820 - val_loss: 0.0929 - lr: 4.8000e-05\n",
            "Epoch 100/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0825 - val_loss: 0.0880 - lr: 4.8000e-05\n",
            "Epoch 101/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0818 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 102/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0815 - val_loss: 0.0888 - lr: 4.8000e-05\n",
            "Epoch 103/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0813 - val_loss: 0.0881 - lr: 9.6000e-06\n",
            "Epoch 104/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0812 - val_loss: 0.0881 - lr: 9.6000e-06\n",
            "Epoch 105/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0812 - val_loss: 0.0877 - lr: 9.6000e-06\n",
            "Epoch 106/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0811 - val_loss: 0.0874 - lr: 9.6000e-06\n",
            "Epoch 107/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0812 - val_loss: 0.0878 - lr: 9.6000e-06\n",
            "Epoch 108/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0810 - val_loss: 0.0874 - lr: 9.6000e-06\n",
            "Epoch 109/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0811 - val_loss: 0.0875 - lr: 9.6000e-06\n",
            "Epoch 110/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0810 - val_loss: 0.0879 - lr: 9.6000e-06\n",
            "Epoch 111/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0811 - val_loss: 0.0875 - lr: 9.6000e-06\n",
            "Epoch 112/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0814 - val_loss: 0.0874 - lr: 9.6000e-06\n",
            "Epoch 113/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0810 - val_loss: 0.0879 - lr: 9.6000e-06\n",
            "Epoch 114/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0809 - val_loss: 0.0874 - lr: 1.9200e-06\n",
            "Epoch 115/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 1.9200e-06\n",
            "Epoch 116/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0809 - val_loss: 0.0873 - lr: 1.9200e-06\n",
            "Epoch 117/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0875 - lr: 1.9200e-06\n",
            "Epoch 118/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0875 - lr: 1.9200e-06\n",
            "Epoch 119/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 1.9200e-06\n",
            "Epoch 120/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 1.9200e-06\n",
            "Epoch 121/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 122/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 123/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 124/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0808 - val_loss: 0.0875 - lr: 3.8400e-07\n",
            "Epoch 125/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 126/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 127/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 3.8400e-07\n",
            "Epoch 128/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 129/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 130/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 131/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 132/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 133/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 134/1000\n",
            "51/51 [==============================] - 1s 25ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 7.6800e-08\n",
            "Epoch 135/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 1.5360e-08\n",
            "Epoch 136/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0874 - lr: 1.5360e-08\n",
            "800/800 [==============================] - 2s 2ms/step\n",
            "Fold 1 NN: 0.08734\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "CV 2/5\n",
            "Epoch 1/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 19.3644 - val_loss: 2.1774 - lr: 0.0060\n",
            "Epoch 2/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 2.9505 - val_loss: 1.0474 - lr: 0.0060\n",
            "Epoch 3/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 3.3291 - val_loss: 0.8657 - lr: 0.0060\n",
            "Epoch 4/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 1.1456 - val_loss: 1.2416 - lr: 0.0060\n",
            "Epoch 5/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 1.1981 - val_loss: 0.8416 - lr: 0.0060\n",
            "Epoch 6/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.8186 - val_loss: 0.9402 - lr: 0.0060\n",
            "Epoch 7/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 1.1437 - val_loss: 0.3727 - lr: 0.0060\n",
            "Epoch 8/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.5286 - val_loss: 0.2925 - lr: 0.0060\n",
            "Epoch 9/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.7644 - val_loss: 0.7344 - lr: 0.0060\n",
            "Epoch 10/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.7310 - val_loss: 0.3127 - lr: 0.0060\n",
            "Epoch 11/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.6955 - val_loss: 0.8361 - lr: 0.0060\n",
            "Epoch 12/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.7134 - val_loss: 0.3629 - lr: 0.0060\n",
            "Epoch 13/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.5293 - val_loss: 0.7007 - lr: 0.0060\n",
            "Epoch 14/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.6670 - val_loss: 0.7815 - lr: 0.0060\n",
            "Epoch 15/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.6818 - val_loss: 0.5693 - lr: 0.0060\n",
            "Epoch 16/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1794 - val_loss: 0.1183 - lr: 0.0012\n",
            "Epoch 17/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1141 - val_loss: 0.1133 - lr: 0.0012\n",
            "Epoch 18/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1094 - val_loss: 0.1111 - lr: 0.0012\n",
            "Epoch 19/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1076 - val_loss: 0.1143 - lr: 0.0012\n",
            "Epoch 20/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1082 - val_loss: 0.1081 - lr: 0.0012\n",
            "Epoch 21/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1049 - val_loss: 0.1095 - lr: 0.0012\n",
            "Epoch 22/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1037 - val_loss: 0.1047 - lr: 0.0012\n",
            "Epoch 23/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1024 - val_loss: 0.1083 - lr: 0.0012\n",
            "Epoch 24/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1019 - val_loss: 0.1053 - lr: 0.0012\n",
            "Epoch 25/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1030 - val_loss: 0.1022 - lr: 0.0012\n",
            "Epoch 26/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1039 - val_loss: 0.1058 - lr: 0.0012\n",
            "Epoch 27/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1053 - val_loss: 0.1108 - lr: 0.0012\n",
            "Epoch 28/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1035 - val_loss: 0.1035 - lr: 0.0012\n",
            "Epoch 29/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1107 - val_loss: 0.1153 - lr: 0.0012\n",
            "Epoch 30/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1120 - val_loss: 0.1131 - lr: 0.0012\n",
            "Epoch 31/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1448 - val_loss: 0.1985 - lr: 0.0012\n",
            "Epoch 32/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1210 - val_loss: 0.0982 - lr: 0.0012\n",
            "Epoch 33/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1286 - val_loss: 0.2623 - lr: 0.0012\n",
            "Epoch 34/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1951 - val_loss: 0.2165 - lr: 0.0012\n",
            "Epoch 35/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1908 - val_loss: 0.2019 - lr: 0.0012\n",
            "Epoch 36/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1857 - val_loss: 0.2395 - lr: 0.0012\n",
            "Epoch 37/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1163 - val_loss: 0.1164 - lr: 0.0012\n",
            "Epoch 38/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1035 - val_loss: 0.0968 - lr: 0.0012\n",
            "Epoch 39/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1004 - val_loss: 0.0996 - lr: 0.0012\n",
            "Epoch 40/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1033 - val_loss: 0.1188 - lr: 0.0012\n",
            "Epoch 41/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1005 - val_loss: 0.1003 - lr: 0.0012\n",
            "Epoch 42/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.1008 - val_loss: 0.1070 - lr: 0.0012\n",
            "Epoch 43/1000\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 0.1099 - val_loss: 0.0998 - lr: 0.0012\n",
            "Epoch 44/1000\n",
            "50/50 [==============================] - 3s 58ms/step - loss: 0.1038 - val_loss: 0.1158 - lr: 0.0012\n",
            "Epoch 45/1000\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.1079 - val_loss: 0.0945 - lr: 0.0012\n",
            "Epoch 46/1000\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.1044 - val_loss: 0.1060 - lr: 0.0012\n",
            "Epoch 47/1000\n",
            "50/50 [==============================] - 2s 33ms/step - loss: 0.1053 - val_loss: 0.0977 - lr: 0.0012\n",
            "Epoch 48/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.1063 - val_loss: 0.1052 - lr: 0.0012\n",
            "Epoch 49/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1032 - val_loss: 0.1599 - lr: 0.0012\n",
            "Epoch 50/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1068 - val_loss: 0.1159 - lr: 0.0012\n",
            "Epoch 51/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.1091 - val_loss: 0.1240 - lr: 0.0012\n",
            "Epoch 52/1000\n",
            "50/50 [==============================] - 3s 59ms/step - loss: 0.1102 - val_loss: 0.1196 - lr: 0.0012\n",
            "Epoch 53/1000\n",
            "50/50 [==============================] - 3s 51ms/step - loss: 0.0853 - val_loss: 0.0892 - lr: 2.4000e-04\n",
            "Epoch 54/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0807 - val_loss: 0.0893 - lr: 2.4000e-04\n",
            "Epoch 55/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0803 - val_loss: 0.0883 - lr: 2.4000e-04\n",
            "Epoch 56/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0801 - val_loss: 0.0883 - lr: 2.4000e-04\n",
            "Epoch 57/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0809 - val_loss: 0.0910 - lr: 2.4000e-04\n",
            "Epoch 58/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0807 - val_loss: 0.0899 - lr: 2.4000e-04\n",
            "Epoch 59/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0802 - val_loss: 0.0890 - lr: 2.4000e-04\n",
            "Epoch 60/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0799 - val_loss: 0.0883 - lr: 2.4000e-04\n",
            "Epoch 61/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0799 - val_loss: 0.0891 - lr: 2.4000e-04\n",
            "Epoch 62/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0799 - val_loss: 0.0884 - lr: 2.4000e-04\n",
            "Epoch 63/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0790 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 64/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0789 - val_loss: 0.0880 - lr: 4.8000e-05\n",
            "Epoch 65/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0789 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 66/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0790 - val_loss: 0.0877 - lr: 4.8000e-05\n",
            "Epoch 67/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0790 - val_loss: 0.0879 - lr: 4.8000e-05\n",
            "Epoch 68/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0788 - val_loss: 0.0876 - lr: 4.8000e-05\n",
            "Epoch 69/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0788 - val_loss: 0.0877 - lr: 4.8000e-05\n",
            "Epoch 70/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0788 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 71/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0788 - val_loss: 0.0878 - lr: 4.8000e-05\n",
            "Epoch 72/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0787 - val_loss: 0.0876 - lr: 4.8000e-05\n",
            "Epoch 73/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0787 - val_loss: 0.0880 - lr: 4.8000e-05\n",
            "Epoch 74/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0786 - val_loss: 0.0879 - lr: 9.6000e-06\n",
            "Epoch 75/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 9.6000e-06\n",
            "Epoch 76/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0877 - lr: 9.6000e-06\n",
            "Epoch 77/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 9.6000e-06\n",
            "Epoch 78/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0785 - val_loss: 0.0877 - lr: 9.6000e-06\n",
            "Epoch 79/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 9.6000e-06\n",
            "Epoch 80/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0875 - lr: 9.6000e-06\n",
            "Epoch 81/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 9.6000e-06\n",
            "Epoch 82/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 9.6000e-06\n",
            "Epoch 83/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 1.9200e-06\n",
            "Epoch 84/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 1.9200e-06\n",
            "Epoch 85/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0784 - val_loss: 0.0875 - lr: 1.9200e-06\n",
            "Epoch 86/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.9200e-06\n",
            "Epoch 87/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.9200e-06\n",
            "Epoch 88/1000\n",
            "50/50 [==============================] - 1s 26ms/step - loss: 0.0785 - val_loss: 0.0876 - lr: 1.9200e-06\n",
            "Epoch 89/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0785 - val_loss: 0.0875 - lr: 1.9200e-06\n",
            "Epoch 90/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0784 - val_loss: 0.0875 - lr: 3.8400e-07\n",
            "Epoch 91/1000\n",
            "50/50 [==============================] - 2s 33ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 92/1000\n",
            "50/50 [==============================] - 2s 34ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 93/1000\n",
            "50/50 [==============================] - 4s 76ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 94/1000\n",
            "50/50 [==============================] - 6s 113ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 95/1000\n",
            "50/50 [==============================] - 2s 44ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 96/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 3.8400e-07\n",
            "Epoch 97/1000\n",
            "50/50 [==============================] - 2s 35ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 98/1000\n",
            "50/50 [==============================] - 2s 32ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 99/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 100/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 101/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 102/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 103/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 7.6800e-08\n",
            "Epoch 104/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "Epoch 105/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "Epoch 106/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "Epoch 107/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "Epoch 108/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "Epoch 109/1000\n",
            "50/50 [==============================] - 1s 27ms/step - loss: 0.0784 - val_loss: 0.0876 - lr: 1.5360e-08\n",
            "801/801 [==============================] - 2s 2ms/step\n",
            "Fold 2 NN: 0.08752\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "CV 3/5\n",
            "Epoch 1/1000\n",
            "51/51 [==============================] - 2s 31ms/step - loss: 20.0819 - val_loss: 3.9518 - lr: 0.0060\n",
            "Epoch 2/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 1.5188 - val_loss: 1.1504 - lr: 0.0060\n",
            "Epoch 3/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 1.2034 - val_loss: 1.2258 - lr: 0.0060\n",
            "Epoch 4/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.8712 - val_loss: 1.0366 - lr: 0.0060\n",
            "Epoch 5/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.7475 - val_loss: 1.0910 - lr: 0.0060\n",
            "Epoch 6/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6992 - val_loss: 0.3704 - lr: 0.0060\n",
            "Epoch 7/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.5923 - val_loss: 0.6542 - lr: 0.0060\n",
            "Epoch 8/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6085 - val_loss: 0.8216 - lr: 0.0060\n",
            "Epoch 9/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.4889 - val_loss: 0.5723 - lr: 0.0060\n",
            "Epoch 10/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.4602 - val_loss: 0.2406 - lr: 0.0060\n",
            "Epoch 11/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.3023 - val_loss: 0.2458 - lr: 0.0060\n",
            "Epoch 12/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.3717 - val_loss: 0.3998 - lr: 0.0060\n",
            "Epoch 13/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.9923 - val_loss: 5.8735 - lr: 0.0060\n",
            "Epoch 14/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 1.4646 - val_loss: 0.6268 - lr: 0.0060\n",
            "Epoch 15/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.5317 - val_loss: 0.7224 - lr: 0.0060\n",
            "Epoch 16/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.4896 - val_loss: 0.6920 - lr: 0.0060\n",
            "Epoch 17/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.5251 - val_loss: 0.3464 - lr: 0.0060\n",
            "Epoch 18/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1547 - val_loss: 0.1384 - lr: 0.0012\n",
            "Epoch 19/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1161 - val_loss: 0.1187 - lr: 0.0012\n",
            "Epoch 20/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1108 - val_loss: 0.1650 - lr: 0.0012\n",
            "Epoch 21/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1266 - val_loss: 0.1170 - lr: 0.0012\n",
            "Epoch 22/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1085 - val_loss: 0.1223 - lr: 0.0012\n",
            "Epoch 23/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1062 - val_loss: 0.1149 - lr: 0.0012\n",
            "Epoch 24/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1132 - val_loss: 0.1608 - lr: 0.0012\n",
            "Epoch 25/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1160 - val_loss: 0.1194 - lr: 0.0012\n",
            "Epoch 26/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1037 - val_loss: 0.1520 - lr: 0.0012\n",
            "Epoch 27/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1118 - val_loss: 0.1258 - lr: 0.0012\n",
            "Epoch 28/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1026 - val_loss: 0.1031 - lr: 0.0012\n",
            "Epoch 29/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1135 - val_loss: 0.1276 - lr: 0.0012\n",
            "Epoch 30/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1012 - val_loss: 0.1042 - lr: 0.0012\n",
            "Epoch 31/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1002 - val_loss: 0.1571 - lr: 0.0012\n",
            "Epoch 32/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1285 - val_loss: 0.1206 - lr: 0.0012\n",
            "Epoch 33/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0992 - val_loss: 0.1299 - lr: 0.0012\n",
            "Epoch 34/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1418 - val_loss: 0.1837 - lr: 0.0012\n",
            "Epoch 35/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.1220 - val_loss: 0.1242 - lr: 0.0012\n",
            "Epoch 36/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0938 - val_loss: 0.0912 - lr: 2.4000e-04\n",
            "Epoch 37/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0902 - val_loss: 0.0909 - lr: 2.4000e-04\n",
            "Epoch 38/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0900 - val_loss: 0.0905 - lr: 2.4000e-04\n",
            "Epoch 39/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0890 - val_loss: 0.0926 - lr: 2.4000e-04\n",
            "Epoch 40/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0895 - val_loss: 0.0908 - lr: 2.4000e-04\n",
            "Epoch 41/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0893 - val_loss: 0.0905 - lr: 2.4000e-04\n",
            "Epoch 42/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0884 - val_loss: 0.0904 - lr: 2.4000e-04\n",
            "Epoch 43/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0890 - val_loss: 0.1193 - lr: 2.4000e-04\n",
            "Epoch 44/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1015 - val_loss: 0.0928 - lr: 2.4000e-04\n",
            "Epoch 45/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0893 - val_loss: 0.0904 - lr: 2.4000e-04\n",
            "Epoch 46/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0883 - val_loss: 0.0992 - lr: 2.4000e-04\n",
            "Epoch 47/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0901 - val_loss: 0.0917 - lr: 2.4000e-04\n",
            "Epoch 48/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0879 - val_loss: 0.0893 - lr: 2.4000e-04\n",
            "Epoch 49/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0872 - val_loss: 0.0896 - lr: 2.4000e-04\n",
            "Epoch 50/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0879 - val_loss: 0.1094 - lr: 2.4000e-04\n",
            "Epoch 51/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0912 - val_loss: 0.0905 - lr: 2.4000e-04\n",
            "Epoch 52/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0870 - val_loss: 0.0938 - lr: 2.4000e-04\n",
            "Epoch 53/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0876 - val_loss: 0.0891 - lr: 2.4000e-04\n",
            "Epoch 54/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0869 - val_loss: 0.0901 - lr: 2.4000e-04\n",
            "Epoch 55/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0884 - val_loss: 0.0966 - lr: 2.4000e-04\n",
            "Epoch 56/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0882 - val_loss: 0.0941 - lr: 2.4000e-04\n",
            "Epoch 57/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0884 - val_loss: 0.0980 - lr: 2.4000e-04\n",
            "Epoch 58/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0876 - val_loss: 0.0898 - lr: 2.4000e-04\n",
            "Epoch 59/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0869 - val_loss: 0.0880 - lr: 2.4000e-04\n",
            "Epoch 60/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0864 - val_loss: 0.0878 - lr: 2.4000e-04\n",
            "Epoch 61/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0866 - val_loss: 0.1096 - lr: 2.4000e-04\n",
            "Epoch 62/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0878 - val_loss: 0.0891 - lr: 2.4000e-04\n",
            "Epoch 63/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0862 - val_loss: 0.0883 - lr: 2.4000e-04\n",
            "Epoch 64/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0861 - val_loss: 0.0921 - lr: 2.4000e-04\n",
            "Epoch 65/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0862 - val_loss: 0.1304 - lr: 2.4000e-04\n",
            "Epoch 66/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0917 - val_loss: 0.0912 - lr: 2.4000e-04\n",
            "Epoch 67/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0851 - val_loss: 0.1146 - lr: 2.4000e-04\n",
            "Epoch 68/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0885 - val_loss: 0.0879 - lr: 4.8000e-05\n",
            "Epoch 69/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0837 - val_loss: 0.0862 - lr: 4.8000e-05\n",
            "Epoch 70/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0835 - val_loss: 0.0862 - lr: 4.8000e-05\n",
            "Epoch 71/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0835 - val_loss: 0.0870 - lr: 4.8000e-05\n",
            "Epoch 72/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0839 - val_loss: 0.0875 - lr: 4.8000e-05\n",
            "Epoch 73/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0838 - val_loss: 0.0873 - lr: 4.8000e-05\n",
            "Epoch 74/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0838 - val_loss: 0.0861 - lr: 4.8000e-05\n",
            "Epoch 75/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0843 - val_loss: 0.0933 - lr: 4.8000e-05\n",
            "Epoch 76/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0851 - val_loss: 0.0861 - lr: 4.8000e-05\n",
            "Epoch 77/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0837 - val_loss: 0.0868 - lr: 4.8000e-05\n",
            "Epoch 78/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0832 - val_loss: 0.0883 - lr: 4.8000e-05\n",
            "Epoch 79/1000\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.0840 - val_loss: 0.0865 - lr: 4.8000e-05\n",
            "Epoch 80/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0833 - val_loss: 0.0861 - lr: 4.8000e-05\n",
            "Epoch 81/1000\n",
            "51/51 [==============================] - 1s 26ms/step - loss: 0.0832 - val_loss: 0.0880 - lr: 4.8000e-05\n",
            "Epoch 82/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0830 - val_loss: 0.0861 - lr: 9.6000e-06\n",
            "Epoch 83/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0828 - val_loss: 0.0860 - lr: 9.6000e-06\n",
            "Epoch 84/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0829 - val_loss: 0.0874 - lr: 9.6000e-06\n",
            "Epoch 85/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0833 - val_loss: 0.0859 - lr: 9.6000e-06\n",
            "Epoch 86/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0828 - val_loss: 0.0861 - lr: 9.6000e-06\n",
            "Epoch 87/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0827 - val_loss: 0.0863 - lr: 9.6000e-06\n",
            "Epoch 88/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0828 - val_loss: 0.0858 - lr: 9.6000e-06\n",
            "Epoch 89/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0827 - val_loss: 0.0871 - lr: 9.6000e-06\n",
            "Epoch 90/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0830 - val_loss: 0.0861 - lr: 9.6000e-06\n",
            "Epoch 91/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0828 - val_loss: 0.0859 - lr: 9.6000e-06\n",
            "Epoch 92/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0827 - val_loss: 0.0861 - lr: 9.6000e-06\n",
            "Epoch 93/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0827 - val_loss: 0.0862 - lr: 9.6000e-06\n",
            "Epoch 94/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0826 - val_loss: 0.0858 - lr: 9.6000e-06\n",
            "Epoch 95/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0827 - val_loss: 0.0858 - lr: 9.6000e-06\n",
            "Epoch 96/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 1.9200e-06\n",
            "Epoch 97/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0825 - val_loss: 0.0861 - lr: 1.9200e-06\n",
            "Epoch 98/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0825 - val_loss: 0.0860 - lr: 1.9200e-06\n",
            "Epoch 99/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 1.9200e-06\n",
            "Epoch 100/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0861 - lr: 1.9200e-06\n",
            "Epoch 101/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 1.9200e-06\n",
            "Epoch 102/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 1.9200e-06\n",
            "Epoch 103/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 104/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 105/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 106/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 107/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 108/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 109/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 3.8400e-07\n",
            "Epoch 110/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 7.6800e-08\n",
            "Epoch 111/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 7.6800e-08\n",
            "Epoch 112/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 7.6800e-08\n",
            "Epoch 113/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 7.6800e-08\n",
            "Epoch 114/1000\n",
            "51/51 [==============================] - 2s 29ms/step - loss: 0.0825 - val_loss: 0.0859 - lr: 7.6800e-08\n",
            "800/800 [==============================] - 2s 2ms/step\n",
            "Fold 3 NN: 0.08577\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "CV 4/5\n",
            "Epoch 1/1000\n",
            "51/51 [==============================] - 2s 32ms/step - loss: 15.3047 - val_loss: 5.1015 - lr: 0.0060\n",
            "Epoch 2/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 1.7737 - val_loss: 1.2120 - lr: 0.0060\n",
            "Epoch 3/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 1.1996 - val_loss: 1.3657 - lr: 0.0060\n",
            "Epoch 4/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 1.1442 - val_loss: 2.0756 - lr: 0.0060\n",
            "Epoch 5/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 1.1321 - val_loss: 0.8038 - lr: 0.0060\n",
            "Epoch 6/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.7402 - val_loss: 0.2857 - lr: 0.0060\n",
            "Epoch 7/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.4585 - val_loss: 0.9514 - lr: 0.0060\n",
            "Epoch 8/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6608 - val_loss: 0.6007 - lr: 0.0060\n",
            "Epoch 9/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6598 - val_loss: 0.3799 - lr: 0.0060\n",
            "Epoch 10/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6457 - val_loss: 0.6208 - lr: 0.0060\n",
            "Epoch 11/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.4532 - val_loss: 0.7735 - lr: 0.0060\n",
            "Epoch 12/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.6298 - val_loss: 0.9580 - lr: 0.0060\n",
            "Epoch 13/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.6266 - val_loss: 0.5391 - lr: 0.0060\n",
            "Epoch 14/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.1779 - val_loss: 0.1272 - lr: 0.0012\n",
            "Epoch 15/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1125 - val_loss: 0.1660 - lr: 0.0012\n",
            "Epoch 16/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1170 - val_loss: 0.1951 - lr: 0.0012\n",
            "Epoch 17/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.1277 - val_loss: 0.1181 - lr: 0.0012\n",
            "Epoch 18/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1130 - val_loss: 0.1243 - lr: 0.0012\n",
            "Epoch 19/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.1034 - val_loss: 0.1138 - lr: 0.0012\n",
            "Epoch 20/1000\n",
            "51/51 [==============================] - 2s 44ms/step - loss: 0.1083 - val_loss: 0.1135 - lr: 0.0012\n",
            "Epoch 21/1000\n",
            "51/51 [==============================] - 3s 55ms/step - loss: 0.1028 - val_loss: 0.1350 - lr: 0.0012\n",
            "Epoch 22/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1015 - val_loss: 0.2195 - lr: 0.0012\n",
            "Epoch 23/1000\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.1195 - val_loss: 0.2500 - lr: 0.0012\n",
            "Epoch 24/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.1289 - val_loss: 0.1472 - lr: 0.0012\n",
            "Epoch 25/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1127 - val_loss: 0.1240 - lr: 0.0012\n",
            "Epoch 26/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1112 - val_loss: 0.2207 - lr: 0.0012\n",
            "Epoch 27/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.1194 - val_loss: 0.1211 - lr: 0.0012\n",
            "Epoch 28/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0949 - val_loss: 0.0989 - lr: 2.4000e-04\n",
            "Epoch 29/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0902 - val_loss: 0.1018 - lr: 2.4000e-04\n",
            "Epoch 30/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0899 - val_loss: 0.1047 - lr: 2.4000e-04\n",
            "Epoch 31/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0900 - val_loss: 0.1070 - lr: 2.4000e-04\n",
            "Epoch 32/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0908 - val_loss: 0.1155 - lr: 2.4000e-04\n",
            "Epoch 33/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0928 - val_loss: 0.1007 - lr: 2.4000e-04\n",
            "Epoch 34/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0906 - val_loss: 0.1145 - lr: 2.4000e-04\n",
            "Epoch 35/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0933 - val_loss: 0.1079 - lr: 2.4000e-04\n",
            "Epoch 36/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0892 - val_loss: 0.0992 - lr: 4.8000e-05\n",
            "Epoch 37/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0880 - val_loss: 0.0988 - lr: 4.8000e-05\n",
            "Epoch 38/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0875 - val_loss: 0.0988 - lr: 4.8000e-05\n",
            "Epoch 39/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0875 - val_loss: 0.0991 - lr: 4.8000e-05\n",
            "Epoch 40/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0876 - val_loss: 0.0987 - lr: 4.8000e-05\n",
            "Epoch 41/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0873 - val_loss: 0.0986 - lr: 4.8000e-05\n",
            "Epoch 42/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0877 - val_loss: 0.0984 - lr: 4.8000e-05\n",
            "Epoch 43/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0877 - val_loss: 0.0987 - lr: 4.8000e-05\n",
            "Epoch 44/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0876 - val_loss: 0.0976 - lr: 4.8000e-05\n",
            "Epoch 45/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0880 - val_loss: 0.1000 - lr: 4.8000e-05\n",
            "Epoch 46/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0875 - val_loss: 0.0975 - lr: 4.8000e-05\n",
            "Epoch 47/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0875 - val_loss: 0.0998 - lr: 4.8000e-05\n",
            "Epoch 48/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0873 - val_loss: 0.0980 - lr: 4.8000e-05\n",
            "Epoch 49/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0871 - val_loss: 0.1028 - lr: 4.8000e-05\n",
            "Epoch 50/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0879 - val_loss: 0.0986 - lr: 4.8000e-05\n",
            "Epoch 51/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0870 - val_loss: 0.0984 - lr: 4.8000e-05\n",
            "Epoch 52/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0866 - val_loss: 0.0984 - lr: 9.6000e-06\n",
            "Epoch 53/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0865 - val_loss: 0.0985 - lr: 9.6000e-06\n",
            "Epoch 54/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0866 - val_loss: 0.0986 - lr: 9.6000e-06\n",
            "Epoch 55/1000\n",
            "51/51 [==============================] - 1s 27ms/step - loss: 0.0865 - val_loss: 0.0980 - lr: 9.6000e-06\n",
            "Epoch 56/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0867 - val_loss: 0.0981 - lr: 9.6000e-06\n",
            "Epoch 57/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0864 - val_loss: 0.0987 - lr: 9.6000e-06\n",
            "Epoch 58/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0870 - val_loss: 0.0986 - lr: 9.6000e-06\n",
            "Epoch 59/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0864 - val_loss: 0.0985 - lr: 1.9200e-06\n",
            "Epoch 60/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0864 - val_loss: 0.0985 - lr: 1.9200e-06\n",
            "Epoch 61/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0864 - val_loss: 0.0985 - lr: 1.9200e-06\n",
            "Epoch 62/1000\n",
            "51/51 [==============================] - 1s 28ms/step - loss: 0.0863 - val_loss: 0.0983 - lr: 1.9200e-06\n",
            "Epoch 63/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0865 - val_loss: 0.0984 - lr: 1.9200e-06\n",
            "Epoch 64/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0863 - val_loss: 0.0983 - lr: 1.9200e-06\n",
            "Epoch 65/1000\n",
            "51/51 [==============================] - 1s 29ms/step - loss: 0.0863 - val_loss: 0.0980 - lr: 1.9200e-06\n",
            "Epoch 66/1000\n",
            "51/51 [==============================] - 2s 30ms/step - loss: 0.0863 - val_loss: 0.0981 - lr: 3.8400e-07\n",
            "800/800 [==============================] - 2s 2ms/step\n",
            "Fold 4 NN: 0.09755\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "CV 5/5\n",
            "Epoch 1/1000\n",
            "50/50 [==============================] - 2s 34ms/step - loss: 26.0326 - val_loss: 1.9270 - lr: 0.0060\n",
            "Epoch 2/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 2.0350 - val_loss: 1.3318 - lr: 0.0060\n",
            "Epoch 3/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.8123 - val_loss: 0.4670 - lr: 0.0060\n",
            "Epoch 4/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.6357 - val_loss: 0.5244 - lr: 0.0060\n",
            "Epoch 5/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.7750 - val_loss: 0.8942 - lr: 0.0060\n",
            "Epoch 6/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.6876 - val_loss: 0.6079 - lr: 0.0060\n",
            "Epoch 7/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.6951 - val_loss: 0.7747 - lr: 0.0060\n",
            "Epoch 8/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.6100 - val_loss: 0.7893 - lr: 0.0060\n",
            "Epoch 9/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.5854 - val_loss: 0.6983 - lr: 0.0060\n",
            "Epoch 10/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.4272 - val_loss: 0.8435 - lr: 0.0060\n",
            "Epoch 11/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.2297 - val_loss: 0.1421 - lr: 0.0012\n",
            "Epoch 12/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.1367 - val_loss: 0.1252 - lr: 0.0012\n",
            "Epoch 13/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.1182 - val_loss: 0.1303 - lr: 0.0012\n",
            "Epoch 14/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1192 - val_loss: 0.1219 - lr: 0.0012\n",
            "Epoch 15/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1139 - val_loss: 0.1316 - lr: 0.0012\n",
            "Epoch 16/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1155 - val_loss: 0.1184 - lr: 0.0012\n",
            "Epoch 17/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.1097 - val_loss: 0.1367 - lr: 0.0012\n",
            "Epoch 18/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1108 - val_loss: 0.1154 - lr: 0.0012\n",
            "Epoch 19/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1061 - val_loss: 0.1104 - lr: 0.0012\n",
            "Epoch 20/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1073 - val_loss: 0.1180 - lr: 0.0012\n",
            "Epoch 21/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1102 - val_loss: 0.1212 - lr: 0.0012\n",
            "Epoch 22/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.1090 - val_loss: 0.1338 - lr: 0.0012\n",
            "Epoch 23/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1085 - val_loss: 0.1124 - lr: 0.0012\n",
            "Epoch 24/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.1046 - val_loss: 0.1074 - lr: 0.0012\n",
            "Epoch 25/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1063 - val_loss: 0.1445 - lr: 0.0012\n",
            "Epoch 26/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1680 - val_loss: 0.1824 - lr: 0.0012\n",
            "Epoch 27/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.1788 - val_loss: 0.2060 - lr: 0.0012\n",
            "Epoch 28/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1705 - val_loss: 0.2166 - lr: 0.0012\n",
            "Epoch 29/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1646 - val_loss: 0.1849 - lr: 0.0012\n",
            "Epoch 30/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.1581 - val_loss: 0.2119 - lr: 0.0012\n",
            "Epoch 31/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.1633 - val_loss: 0.1665 - lr: 0.0012\n",
            "Epoch 32/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.1022 - val_loss: 0.1028 - lr: 2.4000e-04\n",
            "Epoch 33/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0924 - val_loss: 0.1010 - lr: 2.4000e-04\n",
            "Epoch 34/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0916 - val_loss: 0.0996 - lr: 2.4000e-04\n",
            "Epoch 35/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0913 - val_loss: 0.0999 - lr: 2.4000e-04\n",
            "Epoch 36/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0911 - val_loss: 0.0993 - lr: 2.4000e-04\n",
            "Epoch 37/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0910 - val_loss: 0.0989 - lr: 2.4000e-04\n",
            "Epoch 38/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0906 - val_loss: 0.0985 - lr: 2.4000e-04\n",
            "Epoch 39/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0904 - val_loss: 0.0986 - lr: 2.4000e-04\n",
            "Epoch 40/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0905 - val_loss: 0.0995 - lr: 2.4000e-04\n",
            "Epoch 41/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0903 - val_loss: 0.0980 - lr: 2.4000e-04\n",
            "Epoch 42/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0904 - val_loss: 0.0980 - lr: 2.4000e-04\n",
            "Epoch 43/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0906 - val_loss: 0.0981 - lr: 2.4000e-04\n",
            "Epoch 44/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0894 - val_loss: 0.0977 - lr: 2.4000e-04\n",
            "Epoch 45/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0892 - val_loss: 0.0979 - lr: 2.4000e-04\n",
            "Epoch 46/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0890 - val_loss: 0.0976 - lr: 2.4000e-04\n",
            "Epoch 47/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0894 - val_loss: 0.0975 - lr: 2.4000e-04\n",
            "Epoch 48/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0891 - val_loss: 0.0976 - lr: 2.4000e-04\n",
            "Epoch 49/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0887 - val_loss: 0.0967 - lr: 2.4000e-04\n",
            "Epoch 50/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0882 - val_loss: 0.0970 - lr: 2.4000e-04\n",
            "Epoch 51/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0881 - val_loss: 0.0968 - lr: 2.4000e-04\n",
            "Epoch 52/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0891 - val_loss: 0.0968 - lr: 2.4000e-04\n",
            "Epoch 53/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0886 - val_loss: 0.0973 - lr: 2.4000e-04\n",
            "Epoch 54/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0879 - val_loss: 0.0962 - lr: 2.4000e-04\n",
            "Epoch 55/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0874 - val_loss: 0.0958 - lr: 2.4000e-04\n",
            "Epoch 56/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0872 - val_loss: 0.0955 - lr: 2.4000e-04\n",
            "Epoch 57/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0870 - val_loss: 0.0963 - lr: 2.4000e-04\n",
            "Epoch 58/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0866 - val_loss: 0.0962 - lr: 2.4000e-04\n",
            "Epoch 59/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0875 - val_loss: 0.0962 - lr: 2.4000e-04\n",
            "Epoch 60/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0863 - val_loss: 0.0966 - lr: 2.4000e-04\n",
            "Epoch 61/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0865 - val_loss: 0.0950 - lr: 2.4000e-04\n",
            "Epoch 62/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0861 - val_loss: 0.0951 - lr: 2.4000e-04\n",
            "Epoch 63/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0856 - val_loss: 0.0946 - lr: 2.4000e-04\n",
            "Epoch 64/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0860 - val_loss: 0.0957 - lr: 2.4000e-04\n",
            "Epoch 65/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0860 - val_loss: 0.0946 - lr: 2.4000e-04\n",
            "Epoch 66/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0867 - val_loss: 0.0947 - lr: 2.4000e-04\n",
            "Epoch 67/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0853 - val_loss: 0.0946 - lr: 2.4000e-04\n",
            "Epoch 68/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0850 - val_loss: 0.0942 - lr: 2.4000e-04\n",
            "Epoch 69/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0853 - val_loss: 0.0936 - lr: 2.4000e-04\n",
            "Epoch 70/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0851 - val_loss: 0.0944 - lr: 2.4000e-04\n",
            "Epoch 71/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0854 - val_loss: 0.0929 - lr: 2.4000e-04\n",
            "Epoch 72/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0840 - val_loss: 0.0932 - lr: 2.4000e-04\n",
            "Epoch 73/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0842 - val_loss: 0.0929 - lr: 2.4000e-04\n",
            "Epoch 74/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0846 - val_loss: 0.0935 - lr: 2.4000e-04\n",
            "Epoch 75/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0836 - val_loss: 0.0927 - lr: 2.4000e-04\n",
            "Epoch 76/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0850 - val_loss: 0.0925 - lr: 2.4000e-04\n",
            "Epoch 77/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0836 - val_loss: 0.0937 - lr: 2.4000e-04\n",
            "Epoch 78/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0834 - val_loss: 0.0923 - lr: 2.4000e-04\n",
            "Epoch 79/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0829 - val_loss: 0.0917 - lr: 2.4000e-04\n",
            "Epoch 80/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0834 - val_loss: 0.0923 - lr: 2.4000e-04\n",
            "Epoch 81/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0842 - val_loss: 0.0916 - lr: 2.4000e-04\n",
            "Epoch 82/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0836 - val_loss: 0.0928 - lr: 2.4000e-04\n",
            "Epoch 83/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0839 - val_loss: 0.0971 - lr: 2.4000e-04\n",
            "Epoch 84/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0841 - val_loss: 0.0964 - lr: 2.4000e-04\n",
            "Epoch 85/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0828 - val_loss: 0.0926 - lr: 2.4000e-04\n",
            "Epoch 86/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0823 - val_loss: 0.0908 - lr: 2.4000e-04\n",
            "Epoch 87/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0827 - val_loss: 0.0921 - lr: 2.4000e-04\n",
            "Epoch 88/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0812 - val_loss: 0.0911 - lr: 2.4000e-04\n",
            "Epoch 89/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0824 - val_loss: 0.0908 - lr: 2.4000e-04\n",
            "Epoch 90/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0813 - val_loss: 0.0911 - lr: 2.4000e-04\n",
            "Epoch 91/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0846 - val_loss: 0.0908 - lr: 2.4000e-04\n",
            "Epoch 92/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0827 - val_loss: 0.0916 - lr: 2.4000e-04\n",
            "Epoch 93/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0821 - val_loss: 0.0930 - lr: 2.4000e-04\n",
            "Epoch 94/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0796 - val_loss: 0.0891 - lr: 4.8000e-05\n",
            "Epoch 95/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0786 - val_loss: 0.0899 - lr: 4.8000e-05\n",
            "Epoch 96/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0787 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 97/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0784 - val_loss: 0.0896 - lr: 4.8000e-05\n",
            "Epoch 98/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0784 - val_loss: 0.0895 - lr: 4.8000e-05\n",
            "Epoch 99/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0784 - val_loss: 0.0889 - lr: 4.8000e-05\n",
            "Epoch 100/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0785 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 101/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0783 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 102/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0782 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 103/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0784 - val_loss: 0.0895 - lr: 4.8000e-05\n",
            "Epoch 104/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0782 - val_loss: 0.0897 - lr: 4.8000e-05\n",
            "Epoch 105/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0783 - val_loss: 0.0893 - lr: 4.8000e-05\n",
            "Epoch 106/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0781 - val_loss: 0.0888 - lr: 4.8000e-05\n",
            "Epoch 107/1000\n",
            "50/50 [==============================] - 1s 28ms/step - loss: 0.0781 - val_loss: 0.0890 - lr: 4.8000e-05\n",
            "Epoch 108/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0781 - val_loss: 0.0894 - lr: 4.8000e-05\n",
            "Epoch 109/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0780 - val_loss: 0.0891 - lr: 4.8000e-05\n",
            "Epoch 110/1000\n",
            "50/50 [==============================] - 2s 29ms/step - loss: 0.0780 - val_loss: 0.0892 - lr: 4.8000e-05\n",
            "Epoch 111/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0779 - val_loss: 0.0890 - lr: 4.8000e-05\n",
            "Epoch 112/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0781 - val_loss: 0.0889 - lr: 4.8000e-05\n",
            "Epoch 113/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0779 - val_loss: 0.0889 - lr: 4.8000e-05\n",
            "Epoch 114/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0774 - val_loss: 0.0887 - lr: 9.6000e-06\n",
            "Epoch 115/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0774 - val_loss: 0.0885 - lr: 9.6000e-06\n",
            "Epoch 116/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0773 - val_loss: 0.0887 - lr: 9.6000e-06\n",
            "Epoch 117/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0773 - val_loss: 0.0885 - lr: 9.6000e-06\n",
            "Epoch 118/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0773 - val_loss: 0.0885 - lr: 9.6000e-06\n",
            "Epoch 119/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0773 - val_loss: 0.0886 - lr: 9.6000e-06\n",
            "Epoch 120/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0773 - val_loss: 0.0886 - lr: 9.6000e-06\n",
            "Epoch 121/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0773 - val_loss: 0.0886 - lr: 9.6000e-06\n",
            "Epoch 122/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0773 - val_loss: 0.0887 - lr: 9.6000e-06\n",
            "Epoch 123/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0772 - val_loss: 0.0886 - lr: 1.9200e-06\n",
            "Epoch 124/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0772 - val_loss: 0.0886 - lr: 1.9200e-06\n",
            "Epoch 125/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0772 - val_loss: 0.0886 - lr: 1.9200e-06\n",
            "Epoch 126/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0772 - val_loss: 0.0885 - lr: 1.9200e-06\n",
            "Epoch 127/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0772 - val_loss: 0.0886 - lr: 1.9200e-06\n",
            "Epoch 128/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0772 - val_loss: 0.0886 - lr: 1.9200e-06\n",
            "Epoch 129/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0772 - val_loss: 0.0885 - lr: 1.9200e-06\n",
            "Epoch 130/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 131/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 132/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 133/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 134/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0772 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 135/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 136/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.8400e-07\n",
            "Epoch 137/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 138/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 139/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 140/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 141/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 142/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 143/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 7.6800e-08\n",
            "Epoch 144/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 145/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 146/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 147/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 148/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 149/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 150/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 1.5360e-08\n",
            "Epoch 151/1000\n",
            "50/50 [==============================] - 2s 31ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.0720e-09\n",
            "Epoch 152/1000\n",
            "50/50 [==============================] - 1s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.0720e-09\n",
            "Epoch 153/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.0720e-09\n",
            "Epoch 154/1000\n",
            "50/50 [==============================] - 1s 29ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.0720e-09\n",
            "Epoch 155/1000\n",
            "50/50 [==============================] - 2s 30ms/step - loss: 0.0771 - val_loss: 0.0885 - lr: 3.0720e-09\n",
            "801/801 [==============================] - 2s 2ms/step\n",
            "Fold 5 NN: 0.08845\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
        "test_nn[target_name] = (test_predictions_nn+predictions_lgb)/2\n",
        "\n",
        "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
        "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
        "\n",
        "display(test_nn[['row_id', target_name]].head(3))\n",
        "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
        "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
        "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "4CJ7MyiQPElX",
        "outputId": "eba7fc39-32fd-4be7-8c2f-153d7aa297be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSPE NN: 1.0 - Folds: [0.2087, 0.21252, 0.20899, 0.21559, 0.21407]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  row_id    target\n",
              "0    0-4  0.001317\n",
              "1   0-32  0.001963\n",
              "2   0-34  0.001963"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9d090e5-6fb4-4a58-942c-c1e5f57e4f91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0-4</td>\n",
              "      <td>0.001317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0-32</td>\n",
              "      <td>0.001963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0-34</td>\n",
              "      <td>0.001963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9d090e5-6fb4-4a58-942c-c1e5f57e4f91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9d090e5-6fb4-4a58-942c-c1e5f57e4f91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9d090e5-6fb4-4a58-942c-c1e5f57e4f91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ffn_pred_wt = [x*2 for x in preds]\n",
        "lgb_pred_wt = [x*1 for x in preds_lgb]\n",
        "\n",
        "wt_pred = []\n",
        "for (x1,x2) in zip(ffn_pred_wt, lgb_pred_wt):\n",
        "    wt_pred.append(x1+x2)\n",
        "    \n",
        "#Calculate the predictions of the ensemble model\n",
        "pred_ensemble = [y/3 for y in wt_pred]\n"
      ],
      "metadata": {
        "id": "trZNUNydY_o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feed forward R2-score\",r2_score(y_test, preds))\n",
        "print(\"Feed forward RMSPE-score\",rmspe(y_test, preds))"
      ],
      "metadata": {
        "id": "DntTFgsmaEEN",
        "outputId": "fa412779-d14c-4100-a2f7-89b2eae080f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feed forward R2-score 0.8423959703162888\n",
            "Feed forward RMSPE-score 0.21407114520188308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmspe_score = rmspe(y_test, pred_ensemble)\n",
        "print(f'Our out of folds RMSPE for Ensemble model is {rmspe_score}')\n",
        "    \n",
        "r2 = r2_score(y_test, pred_ensemble)\n",
        "print(f'Our out of folds R2 score for Ensemble model is {r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtlCFV3CY39D",
        "outputId": "de452059-5acf-41ef-b623-c713bfda48a5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our out of folds RMSPE for Ensemble model is 0.6516549450780308\n",
            "Our out of folds R2 score for Ensemble model is 0.75620164558244748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df=pd.DataFrame(preds)\n",
        "preds_df.to_csv('submission_feedforward.csv',index = False)"
      ],
      "metadata": {
        "id": "9H68UgiMOJqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS0O7RSyOMZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}